{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import albumentations as albu\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualzie(**images):\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(\"\".join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# データの拡張用\n",
    "def get_training_augmentation():\n",
    "    IMAGE_SIZE = 256\n",
    "    train_transform = [\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, border_mode=0),\n",
    "        albu.PadIfNeeded(min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=IMAGE_SIZE, width=IMAGE_SIZE, always_apply=True),\n",
    "        albu.IAAAdditiveGaussianNoise(p=0.2),\n",
    "        albu.IAAPerspective(p=0.5),\n",
    "\n",
    "        albu.OneOf([\n",
    "            albu.CLAHE(p=1),\n",
    "            albu.RandomBrightness(p=1),\n",
    "            albu.RandomGamma(p=1),\n",
    "        ], p=0.9),\n",
    "\n",
    "        albu.OneOf([\n",
    "            albu.IAASharpen(p=1),\n",
    "            albu.Blur(blur_limit=3, p=1),\n",
    "            albu.MotionBlur(blur_limit=3, p=1),\n",
    "        ], p=0.9),\n",
    "\n",
    "        albu.OneOf([\n",
    "            albu.RandomContrast(p=1),\n",
    "            albu.HueSaturationValue(p=1)\n",
    "        ], p=0.9)\n",
    "    ]\n",
    "\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "# テンソル化\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "# 前処理\n",
    "def get_preproessing(preprocessing_fn):\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "def crop_to_square(image):\n",
    "    size = min(image.size)\n",
    "    left, upper = (image.width - size) // 2, (image.height - size) // 2\n",
    "    right, bottom = (image.width + size) // 2, (image.height + size) // 2\n",
    "    return image.crop((left, upper, right, bottom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old version\n",
    "\n",
    "# class Dataset(BaseDataset):\n",
    "#     CLASSES=[ 'sugarcane', 'weed']\n",
    "    \n",
    "#     def __init__(self, images_dir, masks_dir, classes, augmentation=None, preprocessing=None):\n",
    "#         self.ids = os.listdir(images_dir)\n",
    "#         self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "#         self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "#         # self.class_values =  [classes.index(cls.lower()) for cls in classes]\n",
    "#         self.class_values =  classes\n",
    "#         self.augmentation = augmentation\n",
    "#         self.preprocessing = preprocessing\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "\n",
    "#         image = Image.open(self.images_fps[i])\n",
    "#         image = crop_to_square(image)\n",
    "#         image = image.resize((128, 128), Image.ANTIALIAS)\n",
    "#         image = np.asarray(image)\n",
    "\n",
    "#         masks = Image.open(self.masks_fps[i])\n",
    "#         masks = crop_to_square(masks)\n",
    "#         masks = masks.resize((128, 128), Image.ANTIALIAS)\n",
    "#         masks = np.asarray(masks)\n",
    "\n",
    "#         masks = np.where(masks == 255, 21, masks)\n",
    "\n",
    "#         cls_idx = [self.CLASSES.index(cls) for cls in self.class_values]\n",
    "#         masks = [(masks == idx) for idx in cls_idx]\n",
    "#         mask = np.stack(masks, axis=-1).astype(\"float32\")\n",
    "\n",
    "#         # # 画像データ\n",
    "#         # image = cv2.imread(self.images_fps[i])\n",
    "#         # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         # # マスク画像データ\n",
    "#         # mask = cv2.imread(self.masks_fps[i], 0)\n",
    "#         # masks = [(mask == v) for v in self.class_values]\n",
    "#         # mask = np.stack(masks, axis=-1).astype('float32')\n",
    "\n",
    "#         # データの拡張\n",
    "#         if self.augmentation:\n",
    "#             sample = self.augmentation(image=image, mask=mask)\n",
    "#             image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "#         if self.preprocessing:\n",
    "#             sample = self.preprocessing(image=image, mask=mask)\n",
    "#             image, mask = sample['image'], sample['mask']\n",
    "\n",
    "#         return image, mask\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "\n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    mean=[0.485, 0.456, 0.406]\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "\n",
    "    CLASSES=[ 'sugarcane', 'weed']\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "\n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "        image = t(image)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet34'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = [ 'sugarcane', 'weed']\n",
    "ACTIVATION = 'sigmoid'\n",
    "\n",
    "device = 'cuda'\n",
    "decoder = 'DeepLabV3'\n",
    "\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=ENCODER,\n",
    "#     encoder_weights=ENCODER_WEIGHTS,\n",
    "#     encoder_depth=4,\n",
    "#     decoder_channels=(128, 64, 32, 16),\n",
    "#     classes=len(CLASSES),\n",
    "#     activation=ACTIVATION\n",
    "# )\n",
    "\n",
    "# model = smp.UnetPlusPlus(\n",
    "#     encoder_name=ENCODER,\n",
    "#     encoder_weights=ENCODER_WEIGHTS,\n",
    "#     encoder_depth=4,\n",
    "#     decoder_channels=(128, 64, 32, 16),\n",
    "#     classes=len(CLASSES),\n",
    "#     activation=ACTIVATION\n",
    "# )\n",
    "\n",
    "# model = smp.PSPNet(\n",
    "#     encoder_name=ENCODER,\n",
    "#     encoder_weights=ENCODER_WEIGHTS,\n",
    "#     classes=len(CLASSES),\n",
    "#     activation=ACTIVATION\n",
    "# )\n",
    "\n",
    "# model = smp.DeepLabV3Plus(\n",
    "#     encoder_name=ENCODER,\n",
    "#     encoder_weights=ENCODER_WEIGHTS,\n",
    "#     classes=len(CLASSES),\n",
    "#     activation=ACTIVATION\n",
    "# )\n",
    "\n",
    "model = smp.DeepLabV3(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    classes=len(CLASSES),\n",
    "    activation=ACTIVATION\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"train\"\n",
    "val_dir = \"val\"\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(train_dir + '/images')\n",
    "    os.mkdir(train_dir + '/masks')\n",
    "\n",
    "if not os.path.exists(val_dir):\n",
    "    os.mkdir(val_dir)\n",
    "    os.mkdir(val_dir + '/images')\n",
    "    os.mkdir(val_dir + '/masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './train'\n",
    "val_dir = './val'\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    os.path.join(train_dir, 'images'),\n",
    "    os.path.join(train_dir, 'masks'),\n",
    "    augmentation=get_training_augmentation(),\n",
    "    # preprocessing=get_preproessing(preprocessing_fn),\n",
    "    classes=CLASSES\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    os.path.join(val_dir, 'images'),\n",
    "    os.path.join(val_dir, 'masks'),\n",
    "    augmentation=get_training_augmentation(),\n",
    "    # preprocessing=get_preproessing(preprocessing_fn),\n",
    "    classes=CLASSES\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(os.path.join(train_dir, 'images'), os.path.join(train_dir, 'masks'), classes=[ 'sugarcane', 'weed'])\n",
    "\n",
    "image, mask = dataset[5]\n",
    "\n",
    "print(image.shape)\n",
    "print(mask.shape)\n",
    "\n",
    "visualzie(image=image.permute(1, 2, 0), mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# とりあえず適当な指標を入れてみる\n",
    "\n",
    "metrics = [smp.utils.metrics.IoU(threshold=0.5)]\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.parameters(), lr=0.001),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=device,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "\n",
    "patience = 5\n",
    "early_stop_counter = 0\n",
    "\n",
    "epoch = 50\n",
    "for i in range(epoch):\n",
    "    print(f'\\nEpoch: {i}')\n",
    "    # try:\n",
    "    #     train_logs = train_epoch.run(train_loader)\n",
    "    #     val_logs = valid_epoch.run(valid_loader)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     break\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    val_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    if max_score < val_logs['iou_score']:\n",
    "        max_score = val_logs['iou_score']\n",
    "        torch.save(model, f'./model/{decoder}_{ENCODER}.pth')\n",
    "    #     print('Model saved!')\n",
    "    #     early_stop_counter = 0\n",
    "    # else:\n",
    "    #     early_stop_counter += 1\n",
    "    #     print(f\"not improve for {early_stop_counter} Epoch\")\n",
    "    #     if early_stop_counter==patience:\n",
    "    #         print(f\"early stop. Max Score {max_score}\")\n",
    "    #         break\n",
    "\n",
    "    if i == 30:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-4\n",
    "        print('Decrease decoder learning rate to 1e-4!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"max_score: {max_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as albu\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import segmentation_models_pytorch as smp\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES=[ 'sugarcane', 'weed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testDataset(BaseDataset):\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "\n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    CLASSES=[ 'sugarcane', 'weed']\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "\n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        #t = T.Compose([T.ToTensor()])\n",
    "        #image = t(image)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = testDataset(\n",
    "    os.path.join('./val/' 'images'),\n",
    "    os.path.join('./val/', 'masks'),\n",
    "    # augmentation=get_training_augmentation(),\n",
    "    # preprocessing=get_preproessing(preprocessing_fn),\n",
    "    classes=CLASSES\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files = glob.glob('./val/images/*.png')\n",
    "f = val_files[9]\n",
    "\n",
    "palette_image = Image.open(glob.glob('./val/masks/*.png')[9])\n",
    "PALETTE = palette_image.getpalette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(\"./model/DeepLabV3_resnet34.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=len(CLASSES)):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes):\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0:\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union + smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        \n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_mask_miou(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    model.eval()\n",
    "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "\n",
    "    # print(t)\n",
    "    # print(image.shape)\n",
    "\n",
    "    # image = image.transpose(1, 2, 0).astype(\"float32\")\n",
    "\n",
    "    # print(image.shape)\n",
    "\n",
    "    image = t(image)\n",
    "    model.to(device); image=image.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "\n",
    "        output = model(image)\n",
    "        score = mIoU(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    \n",
    "    return masked, score\n",
    "\n",
    "def predict_image_mask_pixel(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    model.eval()\n",
    "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "    image = t(image)\n",
    "    model.to(device)\n",
    "    image = image.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "\n",
    "        output = model(image)\n",
    "        acc = pixel_accuracy(output, mask)\n",
    "        masked = torch.argmax(output, dim=3)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "\n",
    "    return masked, acc\n",
    "\n",
    "def miou_score(model, test_set):\n",
    "    score_iou = []\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        img, mask = test_set[i]\n",
    "        pred_mask, score = predict_image_mask_miou(model, img, mask)\n",
    "        score_iou.append(score)\n",
    "    return score_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_miou = miou_score(best_model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test set miou\", np.mean(mob_miou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_acc_mean(model, test_set):\n",
    "    accuracy = []\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        img, mask = test_set[i]\n",
    "        pred_mask, acc = predict_image_mask_pixel(model, img, mask)\n",
    "        accuracy.append(acc)\n",
    "    return accuracy\n",
    "\n",
    "def pixel_acc(model, image, mask):\n",
    "    pred_mask, acc = predict_image_mask_pixel(model, image, mask)\n",
    "    return acc\n",
    "\n",
    "mob_acc = pixel_acc_mean(best_model, test_dataset)\n",
    "print(f\"Test Set Pixel accuracy {np.mean(mob_acc) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(os.listdir('./val/images/'))):\n",
    "\n",
    "    # image2, mask2 = test_dataset[i]\n",
    "    image2, mask2 = test_dataloader.dataset[i]\n",
    "    pred_mask2, score2 = predict_image_mask_miou(best_model, image2, mask2)\n",
    "    accuracy = pixel_acc(best_model, image2, mask2)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10))\n",
    "    ax1.imshow(image2)\n",
    "    ax1.set_title('Picture')\n",
    "\n",
    "    mask = Image.fromarray(np.uint8(mask2), mode=\"P\")\n",
    "    mask.putpalette(PALETTE)\n",
    "\n",
    "    ax2.imshow(mask)\n",
    "    ax2.set_title('Ground truth')\n",
    "    ax2.set_axis_off()\n",
    "\n",
    "    pred_mask2 = Image.fromarray(np.uint8(pred_mask2), mode=\"P\")\n",
    "    pred_mask2.putpalette(PALETTE)\n",
    "\n",
    "    ax3.imshow(pred_mask2)\n",
    "    ax3.set_title(f'UNet-MobileNet | mIoU {score2:.3f} | acc {accuracy * 100 :.2f}%')\n",
    "    ax3.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prediction(n):\n",
    "\n",
    "    # img, mask = valid_dataset[n]\n",
    "    img, mask = valid_loader.dataset[n]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, tight_layout=True)\n",
    "    \n",
    "    ax[0].imshow(img.permute(1, 2, 0))\n",
    "\n",
    "    # mask = np.argmax(mask, axis=0)\n",
    "    mask = Image.fromarray(np.uint8(mask), mode=\"P\")\n",
    "    mask.putpalette(PALETTE)\n",
    "\n",
    "    ax[1].imshow(mask)\n",
    "\n",
    "    x = torch.tensor(img).unsqueeze(0)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    y = best_model(x.to(device))\n",
    "    y = y[0].cpu().detach().numpy()\n",
    "    y = np.argmax(y, axis=0)\n",
    "\n",
    "    predict_class_img = Image.fromarray(np.uint8(y), mode=\"P\")\n",
    "    predict_class_img.putpalette(PALETTE)\n",
    "    ax[2].imshow(predict_class_img)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(\"./model/unet_plus_plus_resnet34.pth\")\n",
    "best_model.eval()\n",
    "\n",
    "paths = os.listdir('./val/masks/')\n",
    "\n",
    "# 検証データから\"cat\",\"person\"を含む画像を取得\n",
    "idx_dict = {\"both\":[],\"sugarcane\":[],\"weed\":[]}\n",
    "\n",
    "# 該当の対象物があればpathをリストに加える\n",
    "for i in range(len(paths)):\n",
    "\n",
    "    img = np.asarray(Image.open(f\"./val/masks/{paths[i]}\"))\n",
    "    unique_class = np.unique(img)\n",
    "\n",
    "    if 0 in unique_class and 1 in unique_class:\n",
    "        idx_dict[\"both\"].append(i)\n",
    "        \n",
    "    elif 0 in unique_class:\n",
    "        idx_dict[\"sugarcane\"].append(i)\n",
    "        \n",
    "    elif 1 in unique_class:\n",
    "        idx_dict[\"weed\"].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベル毎に実行して結果を確認\n",
    "for label, idx_list in idx_dict.items():\n",
    "    print(\"=\"*30 , label, \"=\"*30)\n",
    "    for i, idx in enumerate(idx_list):\n",
    "        check_prediction(idx)\n",
    "        if i==2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37cba68cc0666ff0500346fbbc272670c42c6c1b2383619b4dcb2ba70df940d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
